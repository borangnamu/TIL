# LLM (Large Language Model) ì™„ì „ ì •ë³µ ê°€ì´ë“œ

![LLM Overview](https://raw.githubusercontent.com/jalammar/jalammar.github.io/master/images/gpt2/gpt2-sizes-hyperparameters-3.png)


## ðŸ“š ëª©ì°¨
1. [LLMì´ëž€ ë¬´ì—‡ì¸ê°€?](#llmì´ëž€-ë¬´ì—‡ì¸ê°€)
2. [LLMì˜ í•µì‹¬ êµ¬ì¡°](#llmì˜-í•µì‹¬-êµ¬ì¡°)
3. [Transformer ì•„í‚¤í…ì²˜](#transformer-ì•„í‚¤í…ì²˜)
4. [í›ˆë ¨ ê³¼ì •](#í›ˆë ¨-ê³¼ì •)
5. [ì£¼ìš” LLM ëª¨ë¸ë“¤](#ì£¼ìš”-llm-ëª¨ë¸ë“¤)
6. [ì‹¤ì œ í™œìš© ì‚¬ë¡€](#ì‹¤ì œ-í™œìš©-ì‚¬ë¡€)
7. [í•œê³„ì™€ ë„ì „ ê³¼ì œ](#í•œê³„ì™€-ë„ì „-ê³¼ì œ)

---

## ðŸ¤– LLMì´ëž€ ë¬´ì—‡ì¸ê°€?

**Large Language Model(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)**ì€ ë°©ëŒ€í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¡œ í›ˆë ¨ëœ ë”¥ëŸ¬ë‹ ëª¨ë¸ë¡œ, ì¸ê°„ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ì–¸ì–´ë¥¼ ì´í•´í•˜ê³  ìƒì„±í•  ìˆ˜ ìžˆëŠ” AI ì‹œìŠ¤í…œìž…ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **ê·œëª¨**: ìˆ˜ì‹­ì–µ ê°œì˜ ë§¤ê°œë³€ìˆ˜ (GPT-3ëŠ” 1750ì–µ ê°œ)
- **ë‹¤ì–‘ì„±**: í…ìŠ¤íŠ¸ ìƒì„±, ë²ˆì—­, ìš”ì•½, ì§ˆì˜ì‘ë‹µ ë“± ë‹¤ì–‘í•œ ìž‘ì—… ìˆ˜í–‰
- **í•™ìŠµ ë°©ì‹**: ë¹„ì§€ë„ í•™ìŠµì„ í†µí•œ íŒ¨í„´ í•™ìŠµ

![Neural Network](https://raw.githubusercontent.com/jalammar/jalammar.github.io/master/images/t/transformer_resideual_layer_norm_3.png)

---

## ðŸ§  LLMì˜ í•µì‹¬ êµ¬ì¡°

### 1. í† í°í™” (Tokenization)
![Tokenization Process](https://raw.githubusercontent.com/jalammar/jalammar.github.io/master/images/gpt2/gpt2-token-embeddings-wte.png)

```
ìž…ë ¥ í…ìŠ¤íŠ¸: "Hello, world!"
í† í°í™” ê²°ê³¼: ["Hello", ",", " world", "!"]
```

### 2. ìž„ë² ë”© (Embedding)
- ê° í† í°ì„ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
- ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ìˆ˜ì¹˜ë¡œ í‘œí˜„

### 3. ì‹ ê²½ë§ ë ˆì´ì–´
- **ì¸ì½”ë”**: ìž…ë ¥ ì´í•´
- **ë””ì½”ë”**: ì¶œë ¥ ìƒì„±
- **ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜**: ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘

---

## ðŸ”„ Transformer ì•„í‚¤í…ì²˜

![Transformer Architecture](https://raw.githubusercontent.com/jalammar/jalammar.github.io/master/images/t/The_transformer_encoder_decoder_stack.png)

### Attention ë©”ì»¤ë‹ˆì¦˜ ì‹œê°í™”
![Self-Attention](https://raw.githubusercontent.com/jalammar/jalammar.github.io/master/images/t/transformer_self-attention_visualization.png)

### Self-Attention ë©”ì»¤ë‹ˆì¦˜
```python
# ê°„ë‹¨í•œ ì–´í…ì…˜ ê°œë…
def attention(query, key, value):
    # ì¿¼ë¦¬ì™€ í‚¤ì˜ ìœ ì‚¬ë„ ê³„ì‚°
    scores = dot_product(query, key)
    # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    weights = softmax(scores)
    # ê°€ì¤‘í•©ìœ¼ë¡œ ìµœì¢… ì¶œë ¥
    output = weighted_sum(weights, value)
    return output
```

### í•µì‹¬ êµ¬ì„± ìš”ì†Œ
1. **Multi-Head Attention**: ì—¬ëŸ¬ ê´€ì ì—ì„œ ë™ì‹œ ë¶„ì„
2. **Position Encoding**: ë‹¨ì–´ì˜ ìœ„ì¹˜ ì •ë³´ ì¸ì½”ë”©
3. **Feed Forward Network**: ë¹„ì„ í˜• ë³€í™˜
4. **Layer Normalization**: ì•ˆì •ì ì¸ í•™ìŠµ

---

## ðŸ“ˆ í›ˆë ¨ ê³¼ì •

### 1. ì‚¬ì „ í›ˆë ¨ (Pre-training)

```mermaid
graph TD
    A[ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„°] --> B[ë°ì´í„° ì „ì²˜ë¦¬]
    B --> C[í† í°í™”]
    C --> D[Transformer ëª¨ë¸]
    D --> E[ë‹¤ìŒ í† í° ì˜ˆì¸¡ í•™ìŠµ]
    E --> F[ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°]
    F --> G[ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸]
    G --> H{ìˆ˜ë ´í–ˆë‚˜?}
    H -->|No| E
    H -->|Yes| I[ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸]
```

```mermaid
graph LR
    A[ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ ë°ì´í„°] --> B[í† í°í™”]
    B --> C[ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ í•™ìŠµ]
    C --> D[ê¸°ë³¸ ì–¸ì–´ ëª¨ë¸]
```

- **ëª©í‘œ**: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (Next Token Prediction)
- **ë°ì´í„°**: ì›¹ í¬ë¡¤ë§, ì±…, ë‰´ìŠ¤ ë“± ìˆ˜ì¡° ê°œì˜ í† í°
- **ì†ì‹¤ í•¨ìˆ˜**: Cross-entropy Loss

### 2. ë¯¸ì„¸ ì¡°ì • (Fine-tuning)
- **ì§€ë„ í•™ìŠµ**: íŠ¹ì • ìž‘ì—…ì— ë§žëŠ” ë°ì´í„°ë¡œ ì¶”ê°€ í›ˆë ¨
- **ê°•í™” í•™ìŠµ**: RLHF (Reinforcement Learning from Human Feedback)

### 3. í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§
```
ìž…ë ¥ ì˜ˆì‹œ:
"ë‹¤ìŒ ë¬¸ìž¥ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”: 'Hello, how are you?'"

ì¶œë ¥:
"ì•ˆë…•í•˜ì„¸ìš”, ì–´ë–»ê²Œ ì§€ë‚´ì„¸ìš”?"
```

---

## ðŸŒŸ ì£¼ìš” LLM ëª¨ë¸ë“¤

| ëª¨ë¸ | ê°œë°œì‚¬ | ë§¤ê°œë³€ìˆ˜ ìˆ˜ | íŠ¹ì§• |
|------|--------|-------------|------|
| **GPT-4** | OpenAI | ~1ì¡° ê°œ | ë©€í‹°ëª¨ë‹¬, ë†’ì€ ì„±ëŠ¥ |
| **Claude** | Anthropic | ë¯¸ê³µê°œ | ì•ˆì „ì„± ì¤‘ì‹œ, ê¸´ ì»¨í…ìŠ¤íŠ¸ |
| **LLaMA 2** | Meta | 7B-70B | ì˜¤í”ˆì†ŒìŠ¤, ë†’ì€ íš¨ìœ¨ì„± |
| **PaLM 2** | Google | 340B | ë‹¤êµ­ì–´ ì§€ì› |
| **GPT-3.5** | OpenAI | 175B | ChatGPT ê¸°ë°˜ |

### ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ë³€í™”
![Model Scaling](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/142_scaling_laws/scaling_laws.png)

```
ë§¤ê°œë³€ìˆ˜ ìˆ˜ â†‘ â†’ ì„±ëŠ¥ â†‘ (í•˜ì§€ë§Œ ë¹„ìš©ë„ â†‘)

1B â†’ 10B â†’ 100B â†’ 1T
ê¸°ë³¸ â†’ í–¥ìƒ â†’ ê³ ì„±ëŠ¥ â†’ ìµœê³  ì„±ëŠ¥
```

---

## ðŸ’¡ ì‹¤ì œ í™œìš© ì‚¬ë¡€

### 1. ì½˜í…ì¸  ìƒì„±
- **ë¸”ë¡œê·¸ ìž‘ì„±**: ì•„ì´ë””ì–´ë¶€í„° ì™„ì„±ëœ ê¸€ê¹Œì§€
- **ì½”ë”© ì§€ì›**: GitHub Copilot, ChatGPT Code Interpreter
- **ì°½ìž‘ í™œë™**: ì†Œì„¤, ì‹œë‚˜ë¦¬ì˜¤, ë§ˆì¼€íŒ… ì¹´í”¼

### 2. ì—…ë¬´ ìžë™í™”
```python
# ì´ë©”ì¼ ìžë™ ìš”ì•½ ì˜ˆì‹œ
def summarize_email(email_content):
    prompt = f"ë‹¤ìŒ ì´ë©”ì¼ì„ 3ì¤„ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{email_content}"
    return llm_api_call(prompt)
```

### 3. êµìœ¡ ë¶„ì•¼
- **ê°œì¸í™” í•™ìŠµ**: í•™ìŠµìž ìˆ˜ì¤€ì— ë§žëŠ” ì„¤ëª…
- **ì–¸ì–´ í•™ìŠµ**: íšŒí™” ì—°ìŠµ, ë¬¸ë²• êµì •
- **ê³¼ì œ ë„ì›€**: ê°œë… ì„¤ëª…, ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

### 4. ê³ ê° ì„œë¹„ìŠ¤
- **ì±—ë´‡**: 24/7 ê³ ê° ì§€ì›
- **FAQ ìžë™í™”**: ë°˜ë³µì ì¸ ì§ˆë¬¸ ì²˜ë¦¬
- **ê°ì • ë¶„ì„**: ê³ ê° í”¼ë“œë°± ë¶„ì„

---

## âš ï¸ í•œê³„ì™€ ë„ì „ ê³¼ì œ

### 1. ê¸°ìˆ ì  í•œê³„
- **í• ë£¨ì‹œë„¤ì´ì…˜**: ê·¸ëŸ´ë“¯í•˜ì§€ë§Œ í‹€ë¦° ì •ë³´ ìƒì„±
- **ì»¨í…ìŠ¤íŠ¸ ì œí•œ**: ê¸´ ëŒ€í™”ë‚˜ ë¬¸ì„œ ì²˜ë¦¬ì˜ ì–´ë ¤ì›€
- **ì‹¤ì‹œê°„ ì •ë³´ ë¶€ì¡±**: í•™ìŠµ ì‹œì  ì´í›„ ì •ë³´ ëª¨ë¦„

### 2. ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­
```
íŽ¸í–¥ì„± ë¬¸ì œ:
í›ˆë ¨ ë°ì´í„°ì˜ íŽ¸í–¥ â†’ ëª¨ë¸ ì¶œë ¥ì˜ íŽ¸í–¥
ì˜ˆ: ì„±ë³„, ì¸ì¢…, ë¬¸í™”ì  íŽ¸í–¥

í•´ê²° ë°©ì•ˆ:
- ë‹¤ì–‘í•œ ë°ì´í„° ìˆ˜ì§‘
- ê³µì •ì„± í‰ê°€ ì§€í‘œ
- ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§
```

### 3. ê²½ì œì  ì˜í–¥
- **ê³ ë¹„ìš©**: í›ˆë ¨ê³¼ ìš´ì˜ì— ë§‰ëŒ€í•œ ë¹„ìš©
- **ì—ë„ˆì§€ ì†Œë¹„**: í™˜ê²½ì  ì˜í–¥ ê³ ë ¤ í•„ìš”
- **ì¼ìžë¦¬ ë³€í™”**: ìžë™í™”ë¡œ ì¸í•œ ì§ì—… êµ¬ì¡° ë³€í™”

---

## ðŸš€ ë¯¸ëž˜ ì „ë§

![LLM Evolution](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/150_language_model_training_tips/data_timeline.png)

### ì°¨ì„¸ëŒ€ ê¸°ìˆ 
1. **ë©€í‹°ëª¨ë‹¬ AI**: í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€ + ìŒì„± + ë¹„ë””ì˜¤
2. **ì—ì´ì „íŠ¸ AI**: ë³µìž¡í•œ ìž‘ì—… ìžë™ ìˆ˜í–‰
3. **ê°œì¸í™”**: ê°œë³„ ì‚¬ìš©ìž ë§žì¶¤í˜• ëª¨ë¸
4. **íš¨ìœ¨ì„±**: ë” ìž‘ìœ¼ë©´ì„œë„ ê°•ë ¥í•œ ëª¨ë¸

### ê°œë°œ ë™í–¥
```mermaid
graph TD
    A[í˜„ìž¬: ë²”ìš© LLM] --> B[íŠ¹í™” ëª¨ë¸]
    A --> C[íš¨ìœ¨ì„± ìµœì í™”]
    A --> D[ë©€í‹°ëª¨ë‹¬ í†µí•©]
    B --> E[ë¯¸ëž˜: ì „ë¬¸ê°€ AI]
    C --> E
    D --> E
```


