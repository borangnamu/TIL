
# RAG (Retrieval-Augmented Generation) TIL

## 📚 RAG란 무엇인가?

RAG는 **Retrieval-Augmented Generation**의 약자로, 정보 검색(IR)과 생성 모델을 결합한 아키텍처입니다[^1]. 특정 쿼리에 대한 응답을 생성할 때 관련 정보를 검색하여 그 결과를 생성 과정에 통합하는 방식으로 작동합니다[^1].

## 🤖 LLM의 한계와 RAG의 필요성

### LLM의 주요 특징

- 매우 큰 양의 텍스트 데이터에서 학습된 모델 (GPT-3 등)[^1]
- 수십억 개의 파라미터를 보유하며 다양한 언어 작업에서 뛰어난 성능[^1]
- **특정한 데이터 검색 없이도 다양한 질문에 답변 가능**[^1]


### LLM의 한계점

- **환각(Hallucination) 현상**: 학습한 정보에 해답이 없어도 확률적으로 그럴듯한 응답을 생성[^1]
- **최신 정보 부족**: 학습 데이터의 시점 이후 정보에 대해 부정확한 답변 제공[^1]
- **Auto-regression LLM의 특성**: 이전 단어를 보고 가장 높은 확률의 단어를 예측하므로 일관성 부족[^1]


## 🔍 RAG의 작동 원리

### RAG의 3단계 동작 과정

#### 1. 검색 단계 (Retrieval Phase)

- **목적**: 사용자의 질문에 **가장 관련 있는 정보나 문서를 검색**[^1]
- **동작**: 대규모 문서 데이터베이스에서 키워드 검색, 벡터 검색 등을 활용하여 최적의 결과 탐색[^1]


#### 2. 통합 단계 (Integration Phase)

- **목적**: 검색된 정보를 생성 모델의 입력으로 통합하여 **생성될 답변의 품질을 향상**[^1]
- **동작**: 검색된 정보를 생성 모델이 참조할 수 있는 형태로 변환 및 가공[^1]


#### 3. 생성 단계 (Generation Phase)

- **목적**: 통합된 정보를 바탕으로 **사용자의 질문에 대한 답변을 생성**[^1]
- **동작**: 검색된 정보와 자체 학습된 지식을 조합하여 정확하고 상세한 답변 생성[^1]


## 🏗️ RAG와 LLM의 통합 구조

RAG를 사용하면 사용자 질문에 해당하는 응답 정보를 미리 저장한 Vector Store에서 검색하고, 그 검색 결과를 LLM에 전달하여 요약된 결과를 사용자에게 응답하는 방식으로 작동합니다[^1].

### 도서관 비유

- **RAG = 도서관**: 정보를 미리 저장해 둔 특정한 데이터베이스[^1]
- **LLM = 작가**: 검색된 정보를 바탕으로 답변을 생성하는 역할[^1]


## 💡 RAG vs Fine Tuning 비교

| 구분 | RAG | Fine Tuning |
| :-- | :-- | :-- |
| **비용** | 검색 기능 활용으로 훈련 부담 없음[^1] | 막대한 데이터 학습 비용 필요[^1] |
| **업데이트** | 모델 자체 업데이트 불필요[^1] | 지속적인 재훈련 필요[^1] |
| **전문성** | 외부 자료를 통한 전문성 확보[^1] | 모델 자체의 높은 전문성[^1] |
| **환각 현상** | 외부 신뢰 데이터 참조로 감소[^1] | 훈련 데이터에 의존적[^1] |

## 🌐 RAG의 오프라인 활용

RAG는 인터넷 연결 없이도 작동 가능한 구성이 가능합니다[^1]:

- 내부 데이터베이스 활용
- 로컬 데이터베이스에 필요한 데이터 저장
- 기업 환경에서 내/외부 망 분리 상황에서도 활용 가능


## 🎯 RAG의 장점

1. **정확성 향상**: 외부 신뢰할 수 있는 데이터 참조로 환각 현상 감소[^1]
2. **최신성 확보**: 실시간 정보 검색을 통한 최신 정보 제공[^1]
3. **비용 효율성**: LLM 재훈련 없이 성능 향상 가능[^1]
4. **유연성**: 데이터베이스 업데이트만으로 새로운 정보 반영[^1]

